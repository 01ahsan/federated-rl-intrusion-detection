{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoWLeIevDDIj"
      },
      "outputs": [],
      "source": [
        "1st verstion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "class IntrusionDetectionEnv:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.current_idx = 0\n",
        "        self.state = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_idx = 0\n",
        "        self.state = self.X[self.current_idx]\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 1 if action == self.y[self.current_idx] else -1\n",
        "        self.current_idx += 1\n",
        "        done = self.current_idx >= len(self.X)\n",
        "        next_state = self.X[self.current_idx] if not done else None\n",
        "        return next_state, reward, done\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = []\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        q_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target += self.gamma * np.max(self.model.predict(next_state, verbose=0)[0])\n",
        "            target_f = self.model.predict(state, verbose=0)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "def federated_averaging(weights):\n",
        "    avg_weights = [np.mean([weights[i][j] for i in range(len(weights))], axis=0) for j in range(len(weights[0]))]\n",
        "    return avg_weights\n",
        "\n",
        "data = pd.read_csv(\"TON_IoT.csv\")\n",
        "label_encoder = LabelEncoder()\n",
        "data['label'] = label_encoder.fit_transform(data['label'])\n",
        "\n",
        "X = data.drop('label', axis=1).values\n",
        "y = data['label'].values\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_resampled)\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "num_clients = 10\n",
        "X_partitions = np.array_split(X_train, num_clients)\n",
        "y_partitions = np.array_split(y_train, num_clients)\n",
        "\n",
        "state_size = X_train.shape[1]\n",
        "action_size = len(np.unique(y))\n",
        "global_model = DQNAgent(state_size, action_size).model\n",
        "global_weights = global_model.get_weights()\n",
        "\n",
        "clients = [DQNAgent(state_size, action_size) for _ in range(num_clients)]\n",
        "num_rounds = 10\n",
        "batch_size = 32\n",
        "\n",
        "results_df = pd.DataFrame(columns=[\"Round\", \"Q-value (AVG)\", \"F1-score\", \"Recall\", \"Acc (%)\", \"Cumulative Reward (AVG)\"])\n",
        "\n",
        "for round_num in range(num_rounds):\n",
        "    local_weights = []\n",
        "    cumulative_rewards = []\n",
        "    q_values = []\n",
        "\n",
        "    for client_idx, client in enumerate(clients):\n",
        "        X_client = X_partitions[client_idx]\n",
        "        y_client = y_partitions[client_idx]\n",
        "\n",
        "        env = IntrusionDetectionEnv(X_client, y_client)\n",
        "        client.model.set_weights(global_weights)\n",
        "\n",
        "        state = env.reset().reshape(1, state_size)\n",
        "        total_reward = 0\n",
        "        total_q_value = 0\n",
        "        steps = 0\n",
        "\n",
        "        while True:\n",
        "            action = client.act(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            next_state = next_state.reshape(1, state_size) if next_state is not None else None\n",
        "            client.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            q_value = np.max(client.model.predict(state, verbose=0))\n",
        "            total_q_value += q_value\n",
        "            steps += 1\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "            if len(client.memory) > batch_size:\n",
        "                client.replay(batch_size)\n",
        "\n",
        "        cumulative_rewards.append(total_reward / steps)\n",
        "        q_values.append(total_q_value / steps)\n",
        "        local_weights.append(client.model.get_weights())\n",
        "\n",
        "    global_weights = federated_averaging(local_weights)\n",
        "    global_model.set_weights(global_weights)\n",
        "\n",
        "    y_pred = np.argmax(global_model.predict(X_test, verbose=0), axis=1)\n",
        "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
        "    acc = accuracy_score(y_test, y_pred) * 100\n",
        "\n",
        "    results_df.loc[round_num] = [\n",
        "        round_num + 1,\n",
        "        np.mean(q_values),\n",
        "        f1 * 100,\n",
        "        recall * 100,\n",
        "        acc,\n",
        "        np.mean(cumulative_rewards),\n",
        "    ]\n",
        "\n",
        "    print(f\"Round {round_num + 1}/{num_rounds} complete.\")\n",
        "\n",
        "print(results_df)\n",
        "results_df.to_csv(\"training_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "cKZ12HxSDIej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2nd"
      ],
      "metadata": {
        "id": "VeSCOBsoDKpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score, roc_auc_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "\n",
        "class IntrusionDetectionEnv:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.current_idx = 0\n",
        "        self.state = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_idx = 0\n",
        "        self.state = self.X[self.current_idx]\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 10 if action == self.y[self.current_idx] else -1\n",
        "        self.current_idx += 1\n",
        "        done = self.current_idx >= len(self.X)\n",
        "        next_state = self.X[self.current_idx] if not done else None\n",
        "        return next_state, reward, done\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        q_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        states = np.array([x[0] for x in minibatch])\n",
        "        actions = np.array([x[1] for x in minibatch])\n",
        "        rewards = np.array([x[2] for x in minibatch])\n",
        "        next_states = np.array([x[3] for x in minibatch])\n",
        "        dones = np.array([x[4] for x in minibatch])\n",
        "\n",
        "        targets = self.model.predict(states, verbose=0)\n",
        "        next_q_values = self.target_model.predict(next_states, verbose=0)\n",
        "        targets[range(batch_size), actions] = rewards + self.gamma * np.max(next_q_values, axis=1) * (1 - dones)\n",
        "\n",
        "        self.model.fit(states, targets, batch_size=batch_size, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "def federated_averaging(weights):\n",
        "    avg_weights = [np.mean([weights[i][j] for i in range(len(weights))], axis=0) for j in range(len(weights[0]))]\n",
        "    return avg_weights\n",
        "\n",
        "data = pd.read_csv(\"TON_IoT.csv\")\n",
        "label_encoder = LabelEncoder()\n",
        "data['label'] = label_encoder.fit_transform(data['label'])\n",
        "\n",
        "X = data.drop('label', axis=1).values\n",
        "y = data['label'].values\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_resampled)\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "num_clients = 10\n",
        "X_partitions = np.array_split(X_train, num_clients)\n",
        "y_partitions = np.array_split(y_train, num_clients)\n",
        "\n",
        "state_size = X_train.shape[1]\n",
        "action_size = len(np.unique(y))\n",
        "global_model = DQNAgent(state_size, action_size).model\n",
        "global_weights = global_model.get_weights()\n",
        "\n",
        "clients = [DQNAgent(state_size, action_size) for _ in range(num_clients)]\n",
        "num_rounds = 10\n",
        "batch_size = 32\n",
        "\n",
        "results_df = pd.DataFrame(columns=[\"Round\", \"Q-value (AVG)\", \"F1-score\", \"Recall\", \"Precision\", \"ROC-AUC\", \"Acc (%)\", \"Cumulative Reward (AVG)\"])\n",
        "\n",
        "for round_num in range(num_rounds):\n",
        "    local_weights = []\n",
        "    cumulative_rewards = []\n",
        "    q_values = []\n",
        "\n",
        "    for client_idx, client in enumerate(clients):\n",
        "        X_client = X_partitions[client_idx]\n",
        "        y_client = y_partitions[client_idx]\n",
        "\n",
        "        env = IntrusionDetectionEnv(X_client, y_client)\n",
        "        client.model.set_weights(global_weights)\n",
        "\n",
        "        state = env.reset().reshape(1, state_size)\n",
        "        total_reward = 0\n",
        "        total_q_value = 0\n",
        "        steps = 0\n",
        "\n",
        "        while True:\n",
        "            action = client.act(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            next_state = next_state.reshape(1, state_size) if next_state is not None else None\n",
        "            client.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            q_value = np.max(client.model.predict(state, verbose=0))\n",
        "            total_q_value += q_value\n",
        "            steps += 1\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "            if len(client.memory) > batch_size:\n",
        "                client.replay(batch_size)\n",
        "\n",
        "        cumulative_rewards.append(total_reward / steps)\n",
        "        q_values.append(total_q_value / steps)\n",
        "        local_weights.append(client.model.get_weights())\n",
        "\n",
        "    global_weights = federated_averaging(local_weights)\n",
        "    global_model.set_weights(global_weights)\n",
        "\n",
        "    y_pred = np.argmax(global_model.predict(X_test, verbose=0), axis=1)\n",
        "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
        "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
        "    roc_auc = roc_auc_score(y_test, global_model.predict(X_test, verbose=0), multi_class=\"ovr\")\n",
        "    acc = accuracy_score(y_test, y_pred) * 100\n",
        "\n",
        "    results_df.loc[round_num] = [\n",
        "        round_num + 1,\n",
        "        np.mean(q_values),\n",
        "        f1 * 100,\n",
        "        recall * 100,\n",
        "        precision * 100,\n",
        "        roc_auc * 100,\n",
        "        acc,\n",
        "        np.mean(cumulative_rewards),\n",
        "    ]\n",
        "\n",
        "    print(f\"Round {round_num + 1}/{num_rounds} complete.\")\n",
        "\n",
        "print(results_df)\n",
        "results_df.to_csv(\"training_results.csv\", index=False)"
      ],
      "metadata": {
        "id": "KzzfIgGHDfeB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}