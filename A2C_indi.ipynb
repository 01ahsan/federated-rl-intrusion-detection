{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga7DJX_vZg-V"
      },
      "outputs": [],
      "source": [
        "1st varient"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "class IntrusionDetectionEnv:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.current_idx = 0\n",
        "        self.state = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_idx = 0\n",
        "        self.state = self.X[self.current_idx]\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 1 if action == self.y[self.current_idx] else -1\n",
        "        self.current_idx += 1\n",
        "        done = self.current_idx >= len(self.X)\n",
        "        next_state = self.X[self.current_idx] if not done else None\n",
        "        return next_state, reward, done\n",
        "\n",
        "class A2CAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.gamma = 0.99\n",
        "        self.actor, self.critic = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        inputs = Input(shape=(self.state_size,))\n",
        "        dense = Dense(64, activation='relu')(inputs)\n",
        "        policy = Dense(self.action_size, activation='softmax')(dense)\n",
        "        value = Dense(1, activation='linear')(dense)\n",
        "        actor = Model(inputs, policy)\n",
        "        critic = Model(inputs, value)\n",
        "        actor.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001))\n",
        "        critic.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
        "        return actor, critic\n",
        "\n",
        "    def act(self, state):\n",
        "        policy = self.actor.predict(state, verbose=0)[0]\n",
        "        return np.random.choice(self.action_size, p=policy)\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        value = self.critic.predict(state, verbose=0)\n",
        "        next_value = self.critic.predict(next_state, verbose=0) if not done else 0\n",
        "        target = reward + self.gamma * next_value if not done else reward\n",
        "        advantage = target - value\n",
        "        action_onehot = np.zeros((1, self.action_size))\n",
        "        action_onehot[0][action] = 1\n",
        "        self.actor.fit(state, action_onehot * advantage, verbose=0)\n",
        "        self.critic.fit(state, target, verbose=0)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.actor.get_weights(), self.critic.get_weights()\n",
        "\n",
        "    def set_weights(self, actor_weights, critic_weights):\n",
        "        self.actor.set_weights(actor_weights)\n",
        "        self.critic.set_weights(critic_weights)\n",
        "\n",
        "def federated_averaging(weights):\n",
        "    actor_weights = [w[0] for w in weights]\n",
        "    critic_weights = [w[1] for w in weights]\n",
        "    avg_actor = [np.mean([a[i] for a in actor_weights], axis=0) for i in range(len(actor_weights[0]))]\n",
        "    avg_critic = [np.mean([c[i] for c in critic_weights], axis=0) for i in range(len(critic_weights[0]))]\n",
        "    return avg_actor, avg_critic\n",
        "\n",
        "data = pd.read_csv(\"TON_IoT.csv\")\n",
        "label_encoder = LabelEncoder()\n",
        "data['label'] = label_encoder.fit_transform(data['label'])\n",
        "X = data.drop('label', axis=1).values\n",
        "y = data['label'].values\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_resampled)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_resampled, test_size=0.2, random_state=42)\n",
        "num_clients = 10\n",
        "X_partitions = np.array_split(X_train, num_clients)\n",
        "y_partitions = np.array_split(y_train, num_clients)\n",
        "state_size = X_train.shape[1]\n",
        "action_size = len(np.unique(y))\n",
        "global_agent = A2CAgent(state_size, action_size)\n",
        "global_actor_weights, global_critic_weights = global_agent.get_weights()\n",
        "clients = [A2CAgent(state_size, action_size) for _ in range(num_clients)]\n",
        "num_rounds = 10\n",
        "results_df = pd.DataFrame(columns=[\"Round\", \"F1-score\", \"Recall\", \"Acc (%)\", \"Cumulative Reward (AVG)\"])\n",
        "\n",
        "for round_num in range(num_rounds):\n",
        "    local_weights = []\n",
        "    cumulative_rewards = []\n",
        "    for client_idx, client in enumerate(clients):\n",
        "        X_client = X_partitions[client_idx]\n",
        "        y_client = y_partitions[client_idx]\n",
        "        env = IntrusionDetectionEnv(X_client, y_client)\n",
        "        client.set_weights(global_actor_weights, global_critic_weights)\n",
        "        state = env.reset().reshape(1, state_size)\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        while True:\n",
        "            action = client.act(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            next_state = next_state.reshape(1, state_size) if next_state is not None else np.zeros((1, state_size))\n",
        "            client.train(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "            if done:\n",
        "                break\n",
        "        cumulative_rewards.append(total_reward / steps)\n",
        "        local_weights.append(client.get_weights())\n",
        "    global_actor_weights, global_critic_weights = federated_averaging(local_weights)\n",
        "    global_agent.set_weights(global_actor_weights, global_critic_weights)\n",
        "    y_pred_prob = global_agent.actor.predict(X_test, verbose=0)\n",
        "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
        "    acc = accuracy_score(y_test, y_pred) * 100\n",
        "    results_df.loc[round_num] = [round_num + 1, f1 * 100, recall * 100, acc, np.mean(cumulative_rewards)]\n",
        "\n",
        "print(results_df)\n",
        "results_df.to_csv(\"a2c_training_results.csv\", index=False)"
      ],
      "metadata": {
        "id": "TcoM0_hIZjCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2nd varient"
      ],
      "metadata": {
        "id": "eAamTQrmZkwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "class A2C:\n",
        "    def __init__(self, input_shape, action_space):\n",
        "        self.state_size = input_shape\n",
        "        self.action_size = action_space\n",
        "        self.gamma = 0.99\n",
        "        self.actor = self.build_actor()\n",
        "        self.critic = self.build_critic()\n",
        "        self.actor_opt = tf.keras.optimizers.Adam(0.001)\n",
        "        self.critic_opt = tf.keras.optimizers.Adam(0.002)\n",
        "\n",
        "    def build_actor(self):\n",
        "        inputs = layers.Input(shape=(self.state_size,))\n",
        "        x = layers.Dense(128, activation='relu')(inputs)\n",
        "        x = layers.Dense(64, activation='relu')(x)\n",
        "        outputs = layers.Dense(self.action_size, activation='softmax')(x)\n",
        "        return models.Model(inputs, outputs)\n",
        "\n",
        "    def build_critic(self):\n",
        "        inputs = layers.Input(shape=(self.state_size,))\n",
        "        x = layers.Dense(128, activation='relu')(inputs)\n",
        "        x = layers.Dense(64, activation='relu')(x)\n",
        "        outputs = layers.Dense(1)(x)\n",
        "        return models.Model(inputs, outputs)\n",
        "\n",
        "    def act(self, state):\n",
        "        prob = self.actor(state, training=False).numpy()[0]\n",
        "        return np.random.choice(self.action_size, p=prob)\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        state = tf.convert_to_tensor(state)\n",
        "        next_state = tf.convert_to_tensor(next_state)\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            value = self.critic(state)\n",
        "            next_value = self.critic(next_state)\n",
        "            target = reward + (1 - int(done)) * self.gamma * next_value\n",
        "            advantage = target - value\n",
        "            probs = self.actor(state)\n",
        "            action_mask = tf.one_hot([action], self.action_size)\n",
        "            selected_prob = tf.reduce_sum(probs * action_mask, axis=1)\n",
        "            actor_loss = -tf.math.log(selected_prob + 1e-10) * advantage\n",
        "            critic_loss = tf.square(advantage)\n",
        "        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
        "        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
        "        self.actor_opt.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
        "        self.critic_opt.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.actor.get_weights(), self.critic.get_weights()\n",
        "\n",
        "    def set_weights(self, actor_weights, critic_weights):\n",
        "        self.actor.set_weights(actor_weights)\n",
        "        self.critic.set_weights(critic_weights)\n",
        "\n",
        "class Env:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.idx = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.idx = 0\n",
        "        return self.X[self.idx]\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 1 if action == self.y[self.idx] else -1\n",
        "        self.idx += 1\n",
        "        done = self.idx >= len(self.X)\n",
        "        state = self.X[self.idx] if not done else np.zeros_like(self.X[0])\n",
        "        return state, reward, done\n",
        "\n",
        "def average_weights(weights):\n",
        "    actor_weights = [w[0] for w in weights]\n",
        "    critic_weights = [w[1] for w in weights]\n",
        "    avg_actor = [np.mean([a[i] for a in actor_weights], axis=0) for i in range(len(actor_weights[0]))]\n",
        "    avg_critic = [np.mean([c[i] for c in critic_weights], axis=0) for i in range(len(critic_weights[0]))]\n",
        "    return avg_actor, avg_critic\n",
        "\n",
        "data = pd.read_csv(\"TON_IoT.csv\")\n",
        "label_encoder = LabelEncoder()\n",
        "data['label'] = label_encoder.fit_transform(data['label'])\n",
        "X = data.drop('label', axis=1).values\n",
        "y = data['label'].values\n",
        "sm = SMOTE(random_state=42)\n",
        "X, y = sm.fit_resample(X, y)\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "pca = PCA(n_components=0.95)\n",
        "X = pca.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clients_X = np.array_split(X_train, 10)\n",
        "clients_y = np.array_split(y_train, 10)\n",
        "state_dim = X_train.shape[1]\n",
        "action_dim = len(np.unique(y))\n",
        "agent = A2C(state_dim, action_dim)\n",
        "clients = [A2C(state_dim, action_dim) for _ in range(10)]\n",
        "\n",
        "results = pd.DataFrame(columns=[\"Round\", \"F1-score\", \"Recall\", \"Acc (%)\", \"Cumulative Reward (AVG)\"])\n",
        "\n",
        "for rnd in range(10):\n",
        "    weights = []\n",
        "    rewards = []\n",
        "    for i in range(10):\n",
        "        clients[i].set_weights(*agent.get_weights())\n",
        "        env = Env(clients_X[i], clients_y[i])\n",
        "        state = env.reset().reshape(1, -1)\n",
        "        total_reward = 0\n",
        "        step = 0\n",
        "        while True:\n",
        "            action = clients[i].act(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            next_state = next_state.reshape(1, -1)\n",
        "            clients[i].train(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            step += 1\n",
        "            if done:\n",
        "                break\n",
        "        rewards.append(total_reward / step)\n",
        "        weights.append(clients[i].get_weights())\n",
        "    avg_actor, avg_critic = average_weights(weights)\n",
        "    agent.set_weights(avg_actor, avg_critic)\n",
        "    pred = agent.actor.predict(X_test, verbose=0)\n",
        "    pred_label = np.argmax(pred, axis=1)\n",
        "    f1 = f1_score(y_test, pred_label, average='weighted') * 100\n",
        "    recall = recall_score(y_test, pred_label, average='weighted') * 100\n",
        "    acc = accuracy_score(y_test, pred_label) * 100\n",
        "    results.loc[rnd] = [rnd + 1, f1, recall, acc, np.mean(rewards)]\n",
        "\n",
        "print(results)\n",
        "results.to_csv(\"a2c_advanced_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "h2HM82qIZrTl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}